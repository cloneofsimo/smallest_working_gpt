# smallest_working_gpt

Can we make gpt that is fully functional and under 200 lines of code? Yes.

## Why did I make this?

This was used as presentation for beginners who aren't really used to python nor pytorch.
Normally, teaching decoder-only transformer to beginners isn't exactly logical thing to do, but, Republic of Korea is free, democratic country, so I can do whatever I want.

*This 200 lines of code was heavily based on mingpt, which can be found [here](https://github.com/karpathy/minGPT). Some functions were even directly copy - pasted.*

Don't use this for anything serious, as it lacks capability of fine - tuning and everything else.
