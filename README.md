# smallest_working_gpt

Can we make gpt that is fully functional and under 200 lines of code? Yes.

## Why did I make this?

This was used as presentation for beginners who aren't really used to python nor pytorch.
Normally, teaching decoder-only transformer to beginners isn't exactly logical thing to do, but, Republic of Korea is free, democratic country, so I can do whatever I want.
*I have almost no credit to the code, as this was almost make - simpler version of mingpt, which can be found [here](https://github.com/karpathy/minGPT):*

Don't use this for anything serious, as it lacks capability of fine - tuning and everything else.
